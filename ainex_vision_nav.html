<!DOCTYPE HTML>
<!--
  Editorial by HTML5 UP
  html5up.net | @ajlkn
  Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html lang="en">
<head>
  <title>Ainex Vision Navigation | ROS + VLM Decision Loop</title>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
  <meta name="description" content="Ainex humanoid navigation demo: camera snapshot to VLM decision to a single safe ROS motion primitive (safety-first loop)." />
  <link rel="stylesheet" href="assets/css/main.css" />
  <link rel="stylesheet" href="assets/css/project-extras.css" />
  <link rel="icon" href="images/favicon/favicon.ico" type="image/x-icon" />
  <script src="https://cdn.jsdelivr.net/npm/particles.js@2.0.0/particles.min.js"></script>
</head>
<body class="is-preload">
  <a class="skip-link" href="#main-content">Skip to content</a>

  <div id="particles-js"></div>

  <div class="dark-toggle">
    <label class="switch">
      <input type="checkbox" id="toggle-dark-mode">
      <span class="slider round"></span>
    </label>
  </div>

  <script>
  const toggle = document.getElementById('toggle-dark-mode');
  const body = document.body;

  const darkPref = localStorage.getItem('dark-mode') === 'true';
  if (darkPref) {
    body.classList.add('dark-mode');
    if (toggle) toggle.checked = true;
  }

  if (toggle) {
    toggle.addEventListener('change', () => {
      body.classList.toggle('dark-mode', toggle.checked);
      localStorage.setItem('dark-mode', toggle.checked);
    });
  }
</script>

  <div id="wrapper">
    <div id="main">
      <div class="inner" id="main-content">
        <header id="header">
  <a href="index.html" class="logo"><strong>Portfolio</strong> Nandu Krishna R</a>
  <ul class="icons">
    <li><a href="https://www.linkedin.com/in/nandu-krishna-r/" class="icon brands fa-linkedin" aria-label="LinkedIn"><span class="label">LinkedIn</span></a></li>
    <li><a href="https://github.com/Nandu-Kris7" class="icon brands fa-github" aria-label="GitHub"><span class="label">Github</span></a></li>
    <li><a href="https://www.instagram.com/cypher_.7?igsh=NTc4MTIwNjQ2YQ%3D%3D&utm_source=qr" class="icon brands fa-instagram" aria-label="Instagram"><span class="label">Instagram</span></a></li>
  </ul>
</header>

        <section>
  <header class="main">
    <h1>Ainex Vision Navigation</h1>
    <p class="subtitle">Humanoid demo • Vision → decision → one ROS motion primitive (safety-first)</p>
  </header>

  <span class="image project-hero">
    <img src="images/ChatGPT Image Feb 8, 2026, 11_58_48 AM.png" alt="Ainex vision navigation thumbnail" loading="lazy" />
  </span>

  <p style="text-align: justify;">
    I built a safety-first navigation demo for an Ainex humanoid where the robot captures a camera frame, sends it to a
    vision-language decision module, then executes <em>one small, controlled motion primitive</em> (forward/turn/stop) via ROS.
    The point is to separate responsibilities: the AI makes high-level decisions; the robot controller keeps motion stable.
  </p>

  <div class="tldr">
  <div class="box">
    <h3>TL;DR</h3>
    <p style="text-align: justify;">Camera snapshot → VLM decision JSON → exactly one ROS motion primitive. Conservative defaults, predictable behavior, easier debugging.</p>
  </div>
  <div class="box">
    <h3>My role</h3>
    <p style="text-align: justify;">Designed the closed-loop decision loop and safety constraints (single-step execution + STOP fallbacks), integrated ROS motion primitives, and built test tooling for camera capture + command execution.</p>
  </div>
  <div class="box">
    <h3>Tech</h3>
    <div class="tags"><span class="tag">ROS Noetic</span><span class="tag">Python</span><span class="tag">Bash</span><span class="tag">Ainex walking stack</span><span class="tag">Vision-Language Model</span><span class="tag">Linux/Robot workflow</span></div>
  </div>
  <div class="box">
    <h3>Links</h3>
    <div class="actions inline"><a href="https://github.com/Nandu-Kris7/ainex-vision-nav.git" class="button small">GitHub Repo</a></div>
  </div>
</div>

  <hr class="major" />

  <h2>Overview</h2>
  <p style="text-align: justify;">
    This project explores vision-based navigation for an Ainex humanoid robot using ROS (Noetic). The robot captures images onboard,
    sends them to a vision-language decision module, then executes one safe, discrete motion primitive at a time.
    The result is a full autonomy loop — <strong>see → decide → move → see again</strong> — with optional user prompts at ambiguous intersections.
    Conducted as research under Professor Myung (Michael) Cho.
  </p>

  <h2>Workflow</h2>
  <ol>
    <li><strong>Capture:</strong> Save a reliable frame from <code>/camera/image_rect_color</code> (with fallbacks if needed).</li>
    <li><strong>Decide:</strong> Model returns a strict JSON action from a constrained action set.</li>
    <li><strong>Execute:</strong> Run one motion primitive through the Ainex walking stack (ROS topic/service).</li>
    <li><strong>Repeat:</strong> Loop until STOP, or ask user for input at uncertain junctions.</li>
  </ol>

  <h2>Constrained action set</h2>
  <ul>
    <li><code>MOVE_FORWARD_SHORT</code></li>
    <li><code>TURN_LEFT_90</code></li>
    <li><code>TURN_RIGHT_90</code></li>
    <li><code>STOP</code></li>
    <li><code>ASK_USER("Left or right?")</code></li>
  </ul>

  <h2>Design principles</h2>
  <ul>
    <li>The “brain” makes high-level decisions only.</li>
    <li>Low-level walking stability remains inside the existing Ainex controller.</li>
    <li>Default behavior is conservative: when uncertain → STOP or ASK_USER.</li>
  </ul>

  <h2>What I implemented</h2>
  <ul>
    <li><strong>Motion primitives as scripts:</strong> single-purpose commands (forward step, 90° turns, stop).</li>
    <li><strong>Reliable camera capture:</strong> scripts to create a consistent capture image.</li>
    <li><strong>Decision module:</strong> strict JSON parsing + defensive fallbacks to STOP + env-based config (model, tokens, temperature).</li>
    <li><strong>Demo modes:</strong> single-step, closed-loop loop, and <code>DRY_RUN</code> to test decisions without moving hardware.</li>
  </ul>

  <h2>Results / demo behavior</h2>
  <ul>
    <li>Capture frames and perform inference</li>
    <li>Select a discrete action from a constrained action set</li>
    <li>Execute a safe step and re-evaluate the environment</li>
    <li>Support human-in-the-loop navigation: at uncertain junctions, ask for user input rather than guessing</li>
  </ul>

  <h2>Key challenges (and what I learned)</h2>
  <ul>
    <li><strong>ROS environment issues:</strong> mismatched ROS_MASTER_URI, missing nodes, unreachable services → learned fast debugging patterns for ROS networks and startup order.</li>
    <li><strong>Walking drift / curvature:</strong> small gait asymmetries cause arc turns → learned to isolate decision vs execution vs gait tuning.</li>
    <li><strong>Reliability beats cleverness:</strong> the best demo is the one that runs 20 times in a row.</li>
  </ul>

  <hr class="major" />

  <h2>Tech stack</h2>
  <ul>
    <li>ROS Noetic</li>
    <li>Ainex walking stack (topics/services)</li>
    <li>Python (decision loop + client integration)</li>
    <li>Bash scripts (motion/capture primitives)</li>
    <li>Windows dev workflow (GitHub app + sync scripts to robot)</li>
  </ul>

  <h2>Media</h2>
  <div class="media-grid">
    <figure class="media-item">
      <video src="images/Camera testing.mp4" controls></video>
      <figcaption>Camera functionality test on the robot.</figcaption>
    </figure>
    <figure class="media-item">
      <img src="images/camera test cli .png" alt="Camera test CLI screenshot" loading="lazy" class="contain" />
      <figcaption>Camera capture test from the command line.</figcaption>
    </figure>
    <figure class="media-item">
      <iframe src="https://www.youtube.com/embed/9f6ziVSf1DA" title="Ainex vision navigation demo" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
      <figcaption>Demo: closed-loop navigation (capture → decide → execute → repeat).</figcaption>
    </figure>
  </div>

  <hr class="major" />

  <h2>Safety considerations</h2>
  <ul>
    <li>Single-step execution (no continuous uncontrolled walking)</li>
    <li>Conservative defaults: STOP if parsing fails or confidence is low</li>
    <li>Optional user confirmation at ambiguous scenes</li>
    <li>Clear separation between decision and actuation layers</li>
  </ul>

  <h2>Future work</h2>
  <ul>
    <li>Run the “brain” locally on-device (replace API calls with an onboard model)</li>
    <li>Improve gait stability and reduce drift (calibration + parameter tuning + feedback)</li>
    <li>Add simple state (e.g., avoid repeating the same turn immediately)</li>
    <li>Add basic obstacle safety checks (e-stop, distance thresholds, or vision heuristics)</li>
  </ul>

  <hr class="major" />

  <h2>Repository</h2>
  <p style="text-align: justify;">
    Explore the code on GitHub:
    <a href="https://github.com/Nandu-Kris7/ainex-vision-nav.git">Ainex Vision Navigation</a>
  </p>
</section>
      </div>
    </div>

    <div id="sidebar">
  <div class="inner">
    <section id="search" class="alt">
      <form method="post" action="#">
        <input type="text" name="query" id="query" placeholder="Search" />
      </form>
    </section>

    <nav id="menu">
      <header class="major"><h2>Menu</h2></header>
      <ul>
        <li><a href="index.html">Homepage</a></li>
        <li><a href="Resume.html">Resume</a></li>
        <li>
          <span class="opener">Projects</span>
          <ul>
            <li><a href="Risk-X_Real_Time_Risk_Intelligence.html">Risk-X: Real-Time Risk Intelligence</a></li>
            <li><a href="Welkin Aves.html">Welkin Aves</a></li>
            <li><a href="ainex_vision_nav.html">Ainex Vision Navigation</a></li>
            <li><a href="Flight_Price_Prediction.html">Flight Price Prediction</a></li>
            <li><a href="Customer_Spending_Prediction_Using_Machine_Learning.html">Customer Spending Prediction</a></li>
            <li><a href="Drug Classifier.html">Drug Classification</a></li>
            <li><a href="Real-Time Pose-Based Attention Analysis on Raspberry Pi.html">Pose-Based Attention Analysis</a></li>
          </ul>
        </li>
      </ul>
    </nav>

    <section id="contact">
      <header class="major"><h2>Get in touch</h2></header>
      <p style="text-align: justify;">
        Want to collaborate, talk opportunities, or nerd out about a project? Email works best.
      </p>
      <ul class="contact">
        <li class="icon solid fa-envelope"><a href="mailto:nandukris7@outlook.com">nandukris7@outlook.com</a></li>
        <li class="icon brands fa-linkedin"><a href="https://www.linkedin.com/in/nandu-krishna-r/">LinkedIn</a></li>
        <li class="icon brands fa-github"><a href="https://github.com/Nandu-Kris7">GitHub</a></li>
        <li class="icon solid fa-map-marker-alt">Northridge, CA (open to relocate / remote)</li>
      </ul>
    </section>

    <footer id="footer">
      <p class="copyright">&copy; All rights reserved. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
    </footer>
  </div>
</div>
  </div>

  <script src="assets/js/jquery.min.js"></script>
<script src="assets/js/browser.min.js"></script>
<script src="assets/js/breakpoints.min.js"></script>
<script src="assets/js/util.js"></script>
<script src="assets/js/main.js"></script>
  <script>
  particlesJS("particles-js", {
    particles: {
      number: { value: 80, density: { enable: true, value_area: 800 } },
      color: { value: "#330099" },
      shape: { type: "circle" },
      opacity: { value: 0.5 },
      size: { value: 3, random: true },
      line_linked: {
        enable: true,
        distance: 150,
        color: "#33CC99",
        opacity: 0.6,
        width: 2
      },
      move: { enable: true, speed: 2, direction: "none", out_mode: "out" }
    },
    interactivity: {
      detect_on: "canvas",
      events: {
        onhover: { enable: true, mode: "grab" },
        onclick: { enable: true, mode: "push" },
        resize: true
      },
      modes: {
        grab: { distance: 140, line_linked: { opacity: 0.5 } },
        push: { particles_nb: 4 }
      }
    },
    retina_detect: true
  });
</script>
</body>
</html>
